# Text-generator-using-Encoder-Decoder-model

## What is Encoder-Decoder model ?
An encoder-decoder model is a type of neural network architecture commonly used in machine learning tasks that involve sequence-to-sequence mapping. This architecture consists of two main components: an encoder and a decoder.

#### Encoder:
The encoder is responsible for processing the input sequence and transforming it into a fixed-dimensional representation, often called the "context" or "latent representation." This context contains the essential information from the input sequence in a condensed form. Typically, recurrent neural networks (RNNs) or long short-term memory networks (LSTMs) are used as encoder architectures.

#### Decoder:
The decoder takes the fixed-dimensional representation generated by the encoder and uses it to generate the output sequence. It processes this context information and produces a sequence of outputs. Similar to the encoder, recurrent neural networks or other architectures suitable for sequence generation are often used as decoder

## Working

#### Text preprocessing
Import Libraries:
Import TensorFlow and the Tokenizer class from Keras.

Define Input Text:
Create a variable named data that holds your input text. This could be any text data you want to process.

Initialize Tokenizer:
Create an instance of the Tokenizer class called tk.
Fit the tokenizer on the input text using the fit_on_texts method. This step builds a vocabulary based on the words in the text.

Tokenization Loop:
Iterate over each line in the input text (assuming sentences are separated by '\n').
Tokenize each sentence using the texts_to_sequences method of the tokenizer (tk). This method converts each word in the sentence to a numerical index based on the vocabulary learned during fitting.

Create Sequences:
For each tokenized sentence, create sequences of increasing lengths, starting from the first word up to the entire sentence.
Append these sequences to the sequences list.

